{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from autoencoder import Autoencoder\n",
    "from preprocessing import extract_features, get_ngram_frequencies, extract_character_level_representation\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 33\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alexa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ALEXA_1M = '../data/Alexa/top-1m.csv'\n",
    "COLUMN_NAMES_ALEXA = ['ranking', 'domain']\n",
    "\n",
    "df_alexa = pd.read_csv(PATH_ALEXA_1M, names=COLUMN_NAMES_ALEXA)\n",
    "df_alexa = df_alexa.drop(['ranking'], axis='columns')\n",
    "df_alexa['label'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alexa.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AmritaDGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 1 for AmritaDGA samples used for TRAINING\n",
    "PATH_AMRITA_DGA_SET_1 = '../data/AmritaDGA/Task 1/training/training.csv'\n",
    "COLUMN_NAMES_AMRITA_SET_1 = ['domain', 'label']\n",
    "\n",
    "df_amrita_set_1 = pd.read_csv(PATH_AMRITA_DGA_SET_1, names=COLUMN_NAMES_AMRITA_SET_1)\n",
    "\n",
    "\n",
    "# Set 2 for AmritaDGA samples used for TESTING\n",
    "PATH_AMRITA_DGA_SET_2_DOMAINS = '../data/AmritaDGA/Task 1/testing/first testing/test1.txt'\n",
    "PATH_AMRITA_DGA_SET_2_LABELS = '../data/AmritaDGA/Task 1/testing/first testing/test1label.txt'\n",
    "\n",
    "amrita_set_2_domains = pd.read_csv(PATH_AMRITA_DGA_SET_2_DOMAINS, names=['domain'])\n",
    "amrita_set_2_labels = pd.read_csv(PATH_AMRITA_DGA_SET_2_LABELS, names=['label'])\n",
    "df_amrita_set_2 = pd.concat([amrita_set_2_domains, amrita_set_2_labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'AmritaDGA Set 1:', df_amrita_set_1.shape, df_amrita_set_1.query('label == 1').shape, df_amrita_set_1.query('label == 0').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'AmritaDGA Set 2:', df_amrita_set_2.shape, df_amrita_set_2.query('label == 1').shape, df_amrita_set_2.query('label == 0').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSINT (Bambenek Consulting Feeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OSINT = '../data/OSINT/bambenek_dga_feed.txt'\n",
    "COLUMN_NAMES_OSINT = ['domain', 'malware', 'date','link']\n",
    "\n",
    "df_osint = pd.read_csv(PATH_OSINT, skiprows=15, names=COLUMN_NAMES_OSINT)\n",
    "df_osint = df_osint['domain'].to_frame()\n",
    "df_osint['label'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_osint.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexa ----------------------------------------\n",
    "N_TRAIN_SAMPLES_ALEXA = 480000\n",
    "\n",
    "# Separating top 100k to be used to n-gram reputation value feature computation\n",
    "df_alexa_top_100k = df_alexa[:100000]\n",
    "\n",
    "# Obtaining the training samples from Alexa\n",
    "df_train_alexa = df_alexa[100000:100000 + N_TRAIN_SAMPLES_ALEXA].reset_index(drop=True)\n",
    "\n",
    "\n",
    "# AmritaDGA (Set 1) ----------------------------------------\n",
    "df_train_amrita_malicious = df_amrita_set_1.query('label == 1').reset_index(drop=True)\n",
    "df_train_amrita_benign = df_amrita_set_1.query('label == 0').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_AxAm = pd.concat([df_train_alexa, df_train_amrita_malicious], ignore_index=True).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "df_train_AmAm = pd.concat([df_train_amrita_benign, df_train_amrita_malicious], ignore_index=True).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_AxAm.value_counts('label'), df_train_AmAm.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AmritaDGA (Set 2) --------------------------------------\n",
    "N_TEST_SAMPLES_AMRITA = 9000\n",
    "\n",
    "df_test_amrita_benign = df_amrita_set_2.query('label == 0').sample(n=N_TEST_SAMPLES_AMRITA, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "# OSINT --------------------------------------\n",
    "N_TEST_SAMPLES_OSINT = 1000\n",
    "\n",
    "df_osint = df_osint.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "df_test_osint = df_osint[:N_TEST_SAMPLES_OSINT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([df_test_amrita_benign, df_test_osint], ignore_index=True).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'K. H. Park, H. M. Song, J. D. Yoo, S.-Y. Hong, B. Cho, K. Kim, et al., \"Unsupervised malicious domain detection with less labeling effort\", Comput. Secur., vol. 116, May 2022.' does not provides any information if a validation set was used. However, let's consider a validation set to prevent overfitting in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alexa ----------------------------------------\n",
    "N_VAL_SAMPLES_ALEXA = 100000\n",
    "\n",
    "last_train_index_alexa = 100000 + N_TRAIN_SAMPLES_ALEXA\n",
    "df_val_alexa = df_alexa[last_train_index_alexa:last_train_index_alexa+N_VAL_SAMPLES_ALEXA].reset_index(drop=True)\n",
    "\n",
    "# OSINT ----------------------------------------\n",
    "N_VAL_SAMPLES_OSINT = 30000\n",
    "df_val_osint = df_osint[N_TEST_SAMPLES_OSINT:N_TEST_SAMPLES_OSINT + N_VAL_SAMPLES_OSINT].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.concat([df_val_alexa, df_val_osint], ignore_index=True).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.value_counts('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocessing data and extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_frequencies = get_ngram_frequencies(df_alexa_top_100k['domain'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_AxAm_pp = extract_features(df_train_AxAm, 'domain', ngram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_AmAm_pp = extract_features(df_train_AmAm, 'domain', ngram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_pp = extract_features(df_val, 'domain', ngram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_pp = extract_features(df_test, 'domain', ngram_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVE = '../data'\n",
    "\n",
    "if not os.path.exists(PATH_SAVE):\n",
    "    os.makedirs(PATH_SAVE)\n",
    "\n",
    "df_train_AxAm_pp.to_csv(f'{PATH_SAVE}/train_AxAm_preprocessed.csv', index=False)\n",
    "df_train_AmAm_pp.to_csv(f'{PATH_SAVE}/train_AmAm_preprocessed.csv', index=False)\n",
    "df_val_pp.to_csv(f'{PATH_SAVE}/val_preprocessed.csv', index=False)\n",
    "df_test_pp.to_csv(f'{PATH_SAVE}/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting character level features (for baseline comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVE = '../data'\n",
    "df_train_AxAm = pd.read_csv(f'{PATH_SAVE}/train_AxAm_preprocessed.csv', usecols=['domain'])\n",
    "df_train_AmAm = pd.read_csv(f'{PATH_SAVE}/train_AmAm_preprocessed.csv', usecols=['domain'])\n",
    "df_val = pd.read_csv(f'{PATH_SAVE}/val_preprocessed.csv', usecols=['domain'])\n",
    "df_test = pd.read_csv(f'{PATH_SAVE}/test_preprocessed.csv', usecols=['domain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_features_train_AxAm = extract_character_level_representation(df_train_AxAm, domain_col='domain', max_len=256, should_remove_TLD=True)\n",
    "cl_features_train_AmAm = extract_character_level_representation(df_train_AmAm, domain_col='domain', max_len=256, should_remove_TLD=True)\n",
    "cl_features_val = extract_character_level_representation(df_val, domain_col='domain', max_len=256, should_remove_TLD=True)\n",
    "cl_features_test = extract_character_level_representation(df_test, domain_col='domain', max_len=256, should_remove_TLD=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_SAVE = '../data'\n",
    "\n",
    "if not os.path.exists(PATH_SAVE):\n",
    "    os.makedirs(PATH_SAVE)\n",
    "\n",
    "np.save(f'{PATH_SAVE}/train_AxAm_charlevel_features.npy', cl_features_train_AxAm)\n",
    "np.save(f'{PATH_SAVE}/train_AmAm_charlevel_features.npy', cl_features_train_AmAm)\n",
    "np.save(f'{PATH_SAVE}/val_charlevel_features.npy', cl_features_val)\n",
    "np.save(f'{PATH_SAVE}/test_charlevel_features.npy', cl_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ae_dga_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
